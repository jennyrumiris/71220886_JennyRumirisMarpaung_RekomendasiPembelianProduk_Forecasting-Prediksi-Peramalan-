{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Library, Baca dan Load Data\n",
        "Mengimpor semua library yang diperlukan untuk analisis data, pemodelan, dan visualisasi. Library yang digunakan termasuk 'pandas' untuk manipulasi data, 'numpy' untuk operasi numerik, csv supada data csv dapat terbaca, 'matplotlib'untuk visualisasi, statsmodels.tsa.arima.model.ARIMA dari statsmodels digunakan untuk membangun model ARIMA untuk analisis deret waktu, sklearn.metrics.mean_squared_error dari scikit-learn digunakan untuk menghitung mean squared error, yang merupakan metrik evaluasi umum untuk model deret waktu. Membaca data dari file CSV yang bernama 'data.csv'. Menampilkan beberapa baris pertama dari data yang dibaca untuk memastikan bahwa data dimuat dengan benar."
      ],
      "metadata": {
        "id": "DqGniaj24KiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h0rM-_iQjyF8"
      },
      "outputs": [],
      "source": [
        "# Import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import category_encoders as ce"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 2: Membaca file CSV\n",
        "df = pd.read_csv('data.csv')\n",
        "print(df.head(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HqtfZyYkbQ4",
        "outputId": "4c4ce4fe-c9f8-4366-ab29-19e05da2d5b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           USER_ID  PRODUCT_ID  RATING    TIME_STAMP\n",
            "0    AKM1MP6P0OYPR  0132793040     5.0  1.365811e+09\n",
            "1   A2CX7LUOHB2NDG  0321732944     5.0  1.341101e+09\n",
            "2   A2NWSAGRHCP8N5  0439886341     1.0  1.367194e+09\n",
            "3   A2WNBOD3WNDNKT  0439886341     3.0  1.374451e+09\n",
            "4   A1GI0U4ZRJA8WN  0439886341     1.0  1.334707e+09\n",
            "5   A1QGNMC6O1VW39  0511189877     5.0  1.397434e+09\n",
            "6   A3J3BRHTDRFJ2G  0511189877     2.0  1.397434e+09\n",
            "7   A2TY0BTJOTENPG  0511189877     5.0  1.395878e+09\n",
            "8   A34ATBPOK6HCHY  0511189877     5.0  1.395533e+09\n",
            "9    A89DO69P0XZ27  0511189877     5.0  1.395446e+09\n",
            "10   AZYNQZ94U6VDB  0511189877     5.0  1.401322e+09\n",
            "11  A1DA3W4GTFXP6O  0528881469     5.0  1.405642e+09\n",
            "12  A29LPQQDG7LD5J  0528881469     1.0  1.352074e+09\n",
            "13   AO94DHGC771SJ  0528881469     5.0  1.370131e+09\n",
            "14   AMO214LNFCEI4  0528881469     1.0  1.290643e+09\n",
            "15  A28B1G1MSJ6OO1  0528881469     4.0  1.280016e+09\n",
            "16  A3N7T0DY83Y4IG  0528881469     3.0  1.283990e+09\n",
            "17  A1H8PY3QHMQQA0  0528881469     2.0  1.290557e+09\n",
            "18   A2CPBQ5W4OGBX  0528881469     2.0  1.277078e+09\n",
            "19  A265MKAR2WEH3Y  0528881469     4.0  1.294790e+09\n",
            "20  A37K02NKUIT68K  0528881469     5.0  1.293235e+09\n",
            "21  A2AW1SSVUIYV9Y  0528881469     4.0  1.289002e+09\n",
            "22  A2AEHUKOV014BP  0528881469     5.0  1.284250e+09\n",
            "23   AMLFNXUIEMN4T  0528881469     1.0  1.307837e+09\n",
            "24  A2O8FIJR9EBU56  0528881469     4.0  1.278547e+09\n",
            "25  A3IQGFB959IR4P  0528881469     1.0  1.327363e+09\n",
            "26   AYTBGUX49LF3W  0528881469     4.0  1.398470e+09\n",
            "27  A24QFSUU00IZ05  0528881469     2.0  1.293581e+09\n",
            "28  A1NG5X8VYZWX0Q  0528881469     1.0  1.286582e+09\n",
            "29  A1E4WG8HRWWK4R  0528881469     5.0  1.390867e+09\n",
            "30  A2AOEW5UGXFOOQ  0528881469     5.0  1.294790e+09\n",
            "31  A2XSWV6AQI90BR  0528881469     1.0  1.299974e+09\n",
            "32   AR84FMFYCQCWF  0528881469     1.0  1.296691e+09\n",
            "33  A19TBA1WARJS55  0528881469     2.0  1.340496e+09\n",
            "34  A3C5SMBSKKWNPT  0528881469     5.0  1.285114e+09\n",
            "35  A24EV6RXELQZ63  0528881469     1.0  1.317254e+09\n",
            "36  A3T6ZQONABIJSG  0528881469     1.0  1.358035e+09\n",
            "37  A132P6YSJSI5G2  0528881469     2.0  1.298851e+09\n",
            "38  A1NQPG5IJ43HJI  0558835155     3.0  1.372550e+09\n",
            "39  A2WOJCFAWI8VS8  059400232X     5.0  1.379376e+09\n",
            "40  A22FB2WSZSXSHH  059400232X     5.0  1.348790e+09\n",
            "41   AZQZ3STMCBG5H  059400232X     5.0  1.395706e+09\n",
            "42  A2EGPA22UHMQXL  0594012015     1.0  1.405296e+09\n",
            "43   AC57CU3TF6ZMJ  0594012015     5.0  1.404173e+09\n",
            "44  A2YX0Z6RHA8Y2H  0594012015     1.0  1.384301e+09\n",
            "45  A38T51B7J6QVD9  0594012015     1.0  1.377130e+09\n",
            "46  A1HOSS7PNC1LMU  0594012015     1.0  1.374538e+09\n",
            "47   A9HI77BE35VYE  0594012015     1.0  1.373933e+09\n",
            "48  A3HFUWKSPF5QEH  0594012015     5.0  1.374278e+09\n",
            "49  A38ETMHM9ATSF0  0594012015     1.0  1.374106e+09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program tersebut terutama bertujuan untuk membaca, menganalisis, dan melatih model menggunakan data rating, serta memberikan rekomendasi produk berdasarkan model yang sudah dilatih."
      ],
      "metadata": {
        "id": "blraKGm04vrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi kolom TIME_STAMP ke tipe datetime\n",
        "df['TIME_STAMP'] = pd.to_datetime(df['TIME_STAMP'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70ncvelVkhE-",
        "outputId": "801af743-0b2c-4e45-c023-d3be33a87dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program di atas bertujuan untuk memeriksa apakah terdapat nilai null dalam DataFrame df.\n",
        "\n",
        "Pertama-tama, program menggunakan metode isnull() untuk mengidentifikasi nilai null di setiap kolom DataFrame, kemudian menggunakan metode sum() untuk menjumlahkan nilai null di setiap kolom. Hasilnya disimpan dalam variabel null_check. Selanjutnya, program menggunakan metode any() untuk memeriksa apakah terdapat nilai null di DataFrame secara keseluruhan. Jika ada setidaknya satu nilai null, variabel any_null akan bernilai True, jika tidak, maka akan bernilai False.\n",
        "\n",
        "Program kemudian mencetak hasil pemeriksaan dengan menampilkan jumlah nilai null di setiap kolom, dan memberikan pesan apakah terdapat nilai null di DataFrame atau tidak, berdasarkan nilai variabel any_null. Dengan demikian, program ini membantu untuk memastikan keberadaan nilai null dalam DataFrame sehingga dapat diambil langkah-langkah pra-pemrosesan yang sesuai jika diperlukan."
      ],
      "metadata": {
        "id": "du_xnLn844wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memeriksa nilai null\n",
        "null_check = df.isnull().sum()\n",
        "any_null = df.isnull().any().any()\n",
        "\n",
        "print(\"Pemeriksaan nilai null di setiap kolom:\")\n",
        "print(null_check)\n",
        "\n",
        "if any_null:\n",
        "    print(\"\\nAda nilai null di DataFrame.\")\n",
        "else:\n",
        "    print(\"\\nTidak ada nilai null di DataFrame.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWblamemksxs",
        "outputId": "0005b824-f4d1-42df-bf32-6f466cd5157f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pemeriksaan nilai null di setiap kolom:\n",
            "USER_ID              0\n",
            "PRODUCT_ID           1\n",
            "RATING               1\n",
            "TIME_STAMP           1\n",
            "RATING_NORMALIZED    1\n",
            "dtype: int64\n",
            "\n",
            "Ada nilai null di DataFrame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langkah normalisasi data ini dilakukan untuk mengubah rentang nilai kolom RATING agar sesuai dengan rentang yang diinginkan atau untuk membuat data menjadi lebih mudah diinterpretasikan. Dalam kasus ini, nilai-nilai dalam kolom RATING dinormalisasi ke dalam rentang antara 0 dan 1 dengan menggunakan metode Min-Max Scaling. Proses normalisasi dilakukan dengan mengurangi nilai minimum dari setiap nilai dalam kolom RATING, kemudian membaginya dengan selisih antara nilai maksimum dan minimum dari kolom tersebut. Ini membantu menjaga konsistensi dalam rentang nilai di seluruh dataset dan sering digunakan sebelum melakukan proses analisis atau pembuatan model."
      ],
      "metadata": {
        "id": "4maChXP46C5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi data\n",
        "df['RATING_NORMALIZED'] = (df['RATING'] - df['RATING'].min()) / (df['RATING'].max() - df['RATING'].min())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69mDmy5Sk92V",
        "outputId": "48dcc6dc-ef27-4762-a808-c537f9fb2946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langkah ini bertujuan untuk mengubah rating yang sudah dinormalisasi menjadi kelas diskret. Proses ini berguna untuk membuat model yang lebih mudah memahami dan memprediksi, terutama jika terdapat pola tertentu dalam distribusi rating yang dinormalisasi. Dalam kasus ini, nilai-nilai rating yang dinormalisasi dibagi menjadi tiga kelas diskret menggunakan fungsi pd.cut(). Rentang nilai dibagi menjadi tiga interval (bins=3), dan setiap interval diberi label 0, 1, atau 2. Kemudian, kelas-kelas ini diubah menjadi tipe data integer menggunakan metode astype(int). Dengan melakukan ini, kita dapat menggunakan rating sebagai target dalam pembuatan model klasifikasi, di mana model akan memprediksi kelas diskret untuk setiap contoh data."
      ],
      "metadata": {
        "id": "g1Ts6-xx6HR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat target diskret dari rating\n",
        "df['RATING_DISCRETE'] = pd.cut(df['RATING_NORMALIZED'], bins=3, labels=[0, 1, 2]).astype(int)"
      ],
      "metadata": {
        "id": "tQi1735elBO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h-tBS-nO6Ogj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan DataFrame baru\n",
        "print(\"\\nDataFrame baru:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWhQ7_9nlEsd",
        "outputId": "560a4b03-cc00-408c-ba00-7e535582db73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame baru:\n",
            "          USER_ID  PRODUCT_ID  RATING                    TIME_STAMP  \\\n",
            "0   AKM1MP6P0OYPR  0132793040     5.0 1970-01-01 00:00:01.365811200   \n",
            "1  A2CX7LUOHB2NDG  0321732944     5.0 1970-01-01 00:00:01.341100800   \n",
            "2  A2NWSAGRHCP8N5  0439886341     1.0 1970-01-01 00:00:01.367193600   \n",
            "3  A2WNBOD3WNDNKT  0439886341     3.0 1970-01-01 00:00:01.374451200   \n",
            "4  A1GI0U4ZRJA8WN  0439886341     1.0 1970-01-01 00:00:01.334707200   \n",
            "\n",
            "   RATING_NORMALIZED  \n",
            "0           0.729240  \n",
            "1           0.729240  \n",
            "2          -2.135724  \n",
            "3          -0.703242  \n",
            "4          -2.135724  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "menentukan fitur (X) dan target (y) dalam pembuatan model. Fitur (X) terdiri dari kolom-kolom dalam dataframe kecuali 'RATING', 'TIME_STAMP', dan 'RATING_NORMALIZED'. Kolom-kolom tersebut dihapus karena tidak digunakan sebagai fitur dalam model. Target (y) adalah kolom 'RATING_NORMALIZED', yang merupakan hasil normalisasi rating dan digunakan untuk membuat kelas diskret dalam pembuatan model. Dengan menetapkan fitur dan target, kita siap untuk melanjutkan dengan proses pembelajaran mesin."
      ],
      "metadata": {
        "id": "G3xZfHoY6cIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 6: Menentukan fitur dan target\n",
        "X = df.drop(['RATING', 'TIME_STAMP', 'RATING_NORMALIZED'], axis=1)  # Fitur\n",
        "y = df['RATING_NORMALIZED']  # Target"
      ],
      "metadata": {
        "id": "ArlQ7FWtlLoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 7: Membagi data menjadi data latih dan data uji (80% data latih)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Z_F_Jg2MlUi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus kolom 'USER_ID' dan 'PRODUCT_ID'\n",
        "X_train = X_train.drop(['USER_ID', 'PRODUCT_ID'], axis=1)"
      ],
      "metadata": {
        "id": "N5PlUduCm7XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 10: Melatih model Neural Network\n",
        "nn_model = MLPRegressor(random_state=42, max_iter=1000)\n",
        "nn_model.fit(X_train, y_train)\n",
        "\n",
        "# Langkah 11: Menyimpan model Neural Network yang dilatih ke file .pkl\n",
        "joblib.dump(nn_model, 'nn_model.pkl')\n",
        "\n",
        "# Langkah 12: Melatih model Support Vector Machine\n",
        "svm_model = SVR(kernel='rbf')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Langkah 13: Menyimpan model Support Vector Machine yang dilatih ke file .pkl\n",
        "joblib.dump(svm_model, 'svm_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "Er1MJNN6lYcz",
        "outputId": "c81dae52-abe3-42c0-d9d6-b57609c5cf8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "at least one array or dtype is required",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-f750b41d30ba>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Langkah 10: Melatih model Neural Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Langkah 11: Menyimpan model Neural Network yang dilatih ke file .pkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    435\u001b[0m         )\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental, reset)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1614\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    776\u001b[0m         )\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 12: Menyimpan model yang dilatih dengan 100% data latih ke file .pkl\n",
        "joblib.dump(dt_classifier, 'dt_classifier.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7KXY033lbts",
        "outputId": "5d83074b-e839-409f-858b-42a3a240e855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dt_classifier.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Langkah 13: Membagi data menjadi data latih dan data uji (80% data latih)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "TsnNNyGFlep5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 14: Membuat dan melatih model Decision Trees dengan 80% data latih\n",
        "dt_classifier_80 = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier_80.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "1aSJXwbGlfsb",
        "outputId": "d83aad97-8646-459f-fc25-7770e8c05df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "melibatkan menyimpan model Decision Tree yang telah dilatih dengan 80% data latih ke dalam file dengan format .pkl menggunakan modul joblib. Proses ini memungkinkan untuk menyimpan model dalam bentuk yang dapat digunakan kembali di masa mendatang tanpa perlu melatih ulang. Dengan melakukan ini, model dapat dengan mudah diimpor dan digunakan untuk memprediksi pada data baru."
      ],
      "metadata": {
        "id": "BUoWeOtH58B-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 15: Menyimpan model yang dilatih dengan 80% data latih ke file .pkl\n",
        "joblib.dump(dt_classifier_80, 'dt_classifier_80.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elMwx0b8lj2a",
        "outputId": "86a77992-d485-4e4b-b678-cffda4dddc99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dt_classifier_80.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "melibatkan memprediksi kelas untuk data uji menggunakan model Decision Tree yang telah dilatih dengan 80% data latih. Proses ini melibatkan penggunaan model yang telah dilatih untuk membuat prediksi kelas untuk setiap contoh dalam data uji, yang kemudian disimpan dalam variabel y_pred_80. Dengan melakukan ini, kita dapat mengevaluasi kinerja model pada data yang belum pernah dilihat sebelumnya."
      ],
      "metadata": {
        "id": "JbP7QXB_515m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 16: Memprediksi kelas untuk data uji (80%)\n",
        "y_pred_80 = dt_classifier_80.predict(X_test)"
      ],
      "metadata": {
        "id": "lBzqRVWjljsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "menghitung confusion matrix pada data uji. Confusion matrix adalah tabel yang digunakan untuk menggambarkan performa model klasifikasi dengan membandingkan nilai sebenarnya dari kelas target dengan nilai yang diprediksi oleh model. Ini berguna untuk mengevaluasi seberapa baik model dapat mengklasifikasikan data ke dalam kelas yang benar."
      ],
      "metadata": {
        "id": "vqAbZTkf5wTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 17: Menghitung Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_80)\n",
        "print(\"\\nConfusion Matrix pada data uji:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5M3QvHylji4",
        "outputId": "db1a09d3-4071-4d25-dbd1-e2c638588b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix pada data uji:\n",
            "[[20253   418  1390]\n",
            " [  532  7710  1295]\n",
            " [ 1734  1555 86348]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "menghitung laporan klasifikasi (classification report) yang memberikan evaluasi rinci tentang kinerja model pada data uji. Laporan ini mencakup metrik-metrik seperti presisi, recall, dan f1-score untuk setiap kelas target, serta rata-rata presisi, recall, dan f1-score secara keseluruhan. Ini membantu dalam mengevaluasi seberapa baik model dapat mengklasifikasikan data uji.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0CkYiI7j5u94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 18: Menghitung laporan klasifikasi\n",
        "class_report = classification_report(y_test, y_pred_80)\n",
        "print(\"\\nClassification Report pada data uji:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vd2T2-DljXL",
        "outputId": "28c360f5-6ad2-4fca-d720-2c6a367e0eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report pada data uji:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.92      0.91     22061\n",
            "           1       0.80      0.81      0.80      9537\n",
            "           2       0.97      0.96      0.97     89637\n",
            "\n",
            "    accuracy                           0.94    121235\n",
            "   macro avg       0.89      0.90      0.89    121235\n",
            "weighted avg       0.94      0.94      0.94    121235\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi rekomendasi_produk_berdasarkan_prediksi menggunakan model Decision Tree Classifier yang telah dilatih untuk memprediksi rating produk. Dengan fitur-fitur yang telah diencode, fungsi ini menghasilkan rekomendasi lima produk teratas berdasarkan rating prediksi yang dihasilkan oleh model. Ini memberikan cara yang efisien untuk memberikan rekomendasi produk kepada pengguna berdasarkan prediksi model"
      ],
      "metadata": {
        "id": "gegUQH0U5nq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 19: Fungsi Rekomendasi Produk Berdasarkan Prediksi Model\n",
        "def rekomendasi_produk_berdasarkan_prediksi(X, model, top_n=5):\n",
        "    # Membuat prediksi untuk setiap produk\n",
        "    rating_pred = model.predict(X)\n",
        "    # Membuat DataFrame hasil prediksi\n",
        "    pred_df = X.copy()\n",
        "    pred_df['RATING_PREDICTED'] = rating_pred\n",
        "    # Mengelompokkan data berdasarkan PRODUCT_ID dan menghitung rata-rata rating prediksi\n",
        "    rekomendasi = pred_df.groupby('PRODUCT_ID')['RATING_PREDICTED'].mean().sort_values(ascending=False).head(top_n)\n",
        "    return rekomendasi"
      ],
      "metadata": {
        "id": "Cn2Pf6JFljKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pertama-tama, program mencetak pesan yang memberi tahu bahwa rekomendasi produk sedang dibuat menggunakan model yang dilatih dengan 100% data latih.Selanjutnya, program menggunakan fungsi rekomendasi_produk_berdasarkan_prediksi() untuk membuat rekomendasi produk. Fungsi ini mengambil dua parameter: X_encoded, yang merupakan fitur yang sudah diencode, dan dt_classifier, yang merupakan model Decision Tree Classifier yang telah dilatih.\n",
        "\n",
        "Fungsi tersebut membuat prediksi rating untuk setiap produk menggunakan model yang telah dilatih, kemudian mengelompokkan produk berdasarkan PRODUCT_ID dan menghitung rata-rata rating prediksi untuk setiap produk. Hasilnya kemudian diurutkan secara menurun berdasarkan rating prediksi, dan lima produk dengan rating tertinggi dipilih sebagai rekomendasi.Akhirnya, program mencetak rekomendasi produk dalam bentuk DataFrame yang berisi PRODUCT_ID dan rating prediksi untuk setiap produk yang direkomendasikan."
      ],
      "metadata": {
        "id": "h-cWjbwP5V5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan model yang dilatih dengan 100% data latih untuk prediksi\n",
        "print(\"\\nRekomendasi produk berdasarkan prediksi model (dengan 100% data latih):\")\n",
        "print(rekomendasi_produk_berdasarkan_prediksi(X_encoded, dt_classifier))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUy-2btultMR",
        "outputId": "2e636f6f-effa-4b2a-cb1e-f900fb855d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rekomendasi produk berdasarkan prediksi model (dengan 100% data latih):\n",
            "PRODUCT_ID\n",
            "1.998379    2.0\n",
            "1.872807    2.0\n",
            "1.622383    2.0\n",
            "1.736555    2.0\n",
            "1.617216    2.0\n",
            "Name: RATING_PREDICTED, dtype: float64\n"
          ]
        }
      ]
    }
  ]
}