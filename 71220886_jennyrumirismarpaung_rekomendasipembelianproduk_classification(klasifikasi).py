# -*- coding: utf-8 -*-
"""71220886_JennyRumirisMarpaung_RekomendasiPembelianProduk_Classification(Klasifikasi).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xGF1Om6sxWhPfXJKjvX0aCYp72O-mUp2

Import Library, Baca dan Load Data
Mengimpor semua library yang diperlukan untuk analisis data, pemodelan, dan visualisasi. Library yang digunakan termasuk 'pandas' untuk manipulasi data, 'numpy' untuk operasi numerik, csv supada data csv dapat terbaca, 'matplotlib'untuk visualisasi, statsmodels.tsa.arima.model.ARIMA dari statsmodels digunakan untuk membangun model ARIMA untuk analisis deret waktu, sklearn.metrics.mean_squared_error dari scikit-learn digunakan untuk menghitung mean squared error, yang merupakan metrik evaluasi umum untuk model deret waktu. Membaca data dari file CSV yang bernama 'data.csv'. Menampilkan beberapa baris pertama dari data yang dibaca untuk memastikan bahwa data dimuat dengan benar.
"""

# Import library
import pandas as pd
import numpy as np
import csv
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import category_encoders as ce

# Langkah 2: Membaca file CSV
df = pd.read_csv('data.csv')
print(df.head(50))

"""Program tersebut terutama bertujuan untuk membaca, menganalisis, dan melatih model menggunakan data rating, serta memberikan rekomendasi produk berdasarkan model yang sudah dilatih."""

# Konversi kolom TIME_STAMP ke tipe datetime
df['TIME_STAMP'] = pd.to_datetime(df['TIME_STAMP'])

"""Program di atas bertujuan untuk memeriksa apakah terdapat nilai null dalam DataFrame df.

Pertama-tama, program menggunakan metode isnull() untuk mengidentifikasi nilai null di setiap kolom DataFrame, kemudian menggunakan metode sum() untuk menjumlahkan nilai null di setiap kolom. Hasilnya disimpan dalam variabel null_check. Selanjutnya, program menggunakan metode any() untuk memeriksa apakah terdapat nilai null di DataFrame secara keseluruhan. Jika ada setidaknya satu nilai null, variabel any_null akan bernilai True, jika tidak, maka akan bernilai False.

Program kemudian mencetak hasil pemeriksaan dengan menampilkan jumlah nilai null di setiap kolom, dan memberikan pesan apakah terdapat nilai null di DataFrame atau tidak, berdasarkan nilai variabel any_null. Dengan demikian, program ini membantu untuk memastikan keberadaan nilai null dalam DataFrame sehingga dapat diambil langkah-langkah pra-pemrosesan yang sesuai jika diperlukan.
"""

# Memeriksa nilai null
null_check = df.isnull().sum()
any_null = df.isnull().any().any()

print("Pemeriksaan nilai null di setiap kolom:")
print(null_check)

if any_null:
    print("\nAda nilai null di DataFrame.")
else:
    print("\nTidak ada nilai null di DataFrame.")

"""Langkah normalisasi data ini dilakukan untuk mengubah rentang nilai kolom RATING agar sesuai dengan rentang yang diinginkan atau untuk membuat data menjadi lebih mudah diinterpretasikan. Dalam kasus ini, nilai-nilai dalam kolom RATING dinormalisasi ke dalam rentang antara 0 dan 1 dengan menggunakan metode Min-Max Scaling. Proses normalisasi dilakukan dengan mengurangi nilai minimum dari setiap nilai dalam kolom RATING, kemudian membaginya dengan selisih antara nilai maksimum dan minimum dari kolom tersebut. Ini membantu menjaga konsistensi dalam rentang nilai di seluruh dataset dan sering digunakan sebelum melakukan proses analisis atau pembuatan model."""

# Normalisasi data
df['RATING_NORMALIZED'] = (df['RATING'] - df['RATING'].min()) / (df['RATING'].max() - df['RATING'].min())

"""Langkah ini bertujuan untuk mengubah rating yang sudah dinormalisasi menjadi kelas diskret. Proses ini berguna untuk membuat model yang lebih mudah memahami dan memprediksi, terutama jika terdapat pola tertentu dalam distribusi rating yang dinormalisasi. Dalam kasus ini, nilai-nilai rating yang dinormalisasi dibagi menjadi tiga kelas diskret menggunakan fungsi pd.cut(). Rentang nilai dibagi menjadi tiga interval (bins=3), dan setiap interval diberi label 0, 1, atau 2. Kemudian, kelas-kelas ini diubah menjadi tipe data integer menggunakan metode astype(int). Dengan melakukan ini, kita dapat menggunakan rating sebagai target dalam pembuatan model klasifikasi, di mana model akan memprediksi kelas diskret untuk setiap contoh data."""

# Membuat target diskret dari rating
df['RATING_DISCRETE'] = pd.cut(df['RATING_NORMALIZED'], bins=3, labels=[0, 1, 2]).astype(int)

# Menampilkan DataFrame baru
print("\nDataFrame baru:")
print(df.head())

"""menentukan fitur (X) dan target (y) dalam pembuatan model. Fitur (X) terdiri dari kolom-kolom dalam dataframe kecuali 'RATING', 'TIME_STAMP', dan 'RATING_NORMALIZED'. Kolom-kolom tersebut dihapus karena tidak digunakan sebagai fitur dalam model. Target (y) adalah kolom 'RATING_NORMALIZED', yang merupakan hasil normalisasi rating dan digunakan untuk membuat kelas diskret dalam pembuatan model. Dengan menetapkan fitur dan target, kita siap untuk melanjutkan dengan proses pembelajaran mesin."""

# Langkah 6: Menentukan fitur dan target
X = df.drop(['RATING', 'TIME_STAMP', 'RATING_NORMALIZED'], axis=1)  # Fitur
y = df['RATING_NORMALIZED']  # Target

# Langkah 7: Membagi data menjadi data latih dan data uji (80% data latih)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Menghapus kolom 'USER_ID' dan 'PRODUCT_ID'
X_train = X_train.drop(['USER_ID', 'PRODUCT_ID'], axis=1)

# Langkah 10: Melatih model Neural Network
nn_model = MLPRegressor(random_state=42, max_iter=1000)
nn_model.fit(X_train, y_train)

# Langkah 11: Menyimpan model Neural Network yang dilatih ke file .pkl
joblib.dump(nn_model, 'nn_model.pkl')

# Langkah 12: Melatih model Support Vector Machine
svm_model = SVR(kernel='rbf')
svm_model.fit(X_train, y_train)

# Langkah 13: Menyimpan model Support Vector Machine yang dilatih ke file .pkl
joblib.dump(svm_model, 'svm_model.pkl')

# Langkah 12: Menyimpan model yang dilatih dengan 100% data latih ke file .pkl
joblib.dump(dt_classifier, 'dt_classifier.pkl')

# Langkah 13: Membagi data menjadi data latih dan data uji (80% data latih)
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Langkah 14: Membuat dan melatih model Decision Trees dengan 80% data latih
dt_classifier_80 = DecisionTreeClassifier(random_state=42)
dt_classifier_80.fit(X_train, y_train)

"""melibatkan menyimpan model Decision Tree yang telah dilatih dengan 80% data latih ke dalam file dengan format .pkl menggunakan modul joblib. Proses ini memungkinkan untuk menyimpan model dalam bentuk yang dapat digunakan kembali di masa mendatang tanpa perlu melatih ulang. Dengan melakukan ini, model dapat dengan mudah diimpor dan digunakan untuk memprediksi pada data baru."""

# Langkah 15: Menyimpan model yang dilatih dengan 80% data latih ke file .pkl
joblib.dump(dt_classifier_80, 'dt_classifier_80.pkl')

"""melibatkan memprediksi kelas untuk data uji menggunakan model Decision Tree yang telah dilatih dengan 80% data latih. Proses ini melibatkan penggunaan model yang telah dilatih untuk membuat prediksi kelas untuk setiap contoh dalam data uji, yang kemudian disimpan dalam variabel y_pred_80. Dengan melakukan ini, kita dapat mengevaluasi kinerja model pada data yang belum pernah dilihat sebelumnya."""

# Langkah 16: Memprediksi kelas untuk data uji (80%)
y_pred_80 = dt_classifier_80.predict(X_test)

"""menghitung confusion matrix pada data uji. Confusion matrix adalah tabel yang digunakan untuk menggambarkan performa model klasifikasi dengan membandingkan nilai sebenarnya dari kelas target dengan nilai yang diprediksi oleh model. Ini berguna untuk mengevaluasi seberapa baik model dapat mengklasifikasikan data ke dalam kelas yang benar."""

# Langkah 17: Menghitung Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_80)
print("\nConfusion Matrix pada data uji:")
print(conf_matrix)

"""menghitung laporan klasifikasi (classification report) yang memberikan evaluasi rinci tentang kinerja model pada data uji. Laporan ini mencakup metrik-metrik seperti presisi, recall, dan f1-score untuk setiap kelas target, serta rata-rata presisi, recall, dan f1-score secara keseluruhan. Ini membantu dalam mengevaluasi seberapa baik model dapat mengklasifikasikan data uji.







"""

# Langkah 18: Menghitung laporan klasifikasi
class_report = classification_report(y_test, y_pred_80)
print("\nClassification Report pada data uji:")
print(class_report)

"""Fungsi rekomendasi_produk_berdasarkan_prediksi menggunakan model Decision Tree Classifier yang telah dilatih untuk memprediksi rating produk. Dengan fitur-fitur yang telah diencode, fungsi ini menghasilkan rekomendasi lima produk teratas berdasarkan rating prediksi yang dihasilkan oleh model. Ini memberikan cara yang efisien untuk memberikan rekomendasi produk kepada pengguna berdasarkan prediksi model"""

# Langkah 19: Fungsi Rekomendasi Produk Berdasarkan Prediksi Model
def rekomendasi_produk_berdasarkan_prediksi(X, model, top_n=5):
    # Membuat prediksi untuk setiap produk
    rating_pred = model.predict(X)
    # Membuat DataFrame hasil prediksi
    pred_df = X.copy()
    pred_df['RATING_PREDICTED'] = rating_pred
    # Mengelompokkan data berdasarkan PRODUCT_ID dan menghitung rata-rata rating prediksi
    rekomendasi = pred_df.groupby('PRODUCT_ID')['RATING_PREDICTED'].mean().sort_values(ascending=False).head(top_n)
    return rekomendasi

"""Pertama-tama, program mencetak pesan yang memberi tahu bahwa rekomendasi produk sedang dibuat menggunakan model yang dilatih dengan 100% data latih.Selanjutnya, program menggunakan fungsi rekomendasi_produk_berdasarkan_prediksi() untuk membuat rekomendasi produk. Fungsi ini mengambil dua parameter: X_encoded, yang merupakan fitur yang sudah diencode, dan dt_classifier, yang merupakan model Decision Tree Classifier yang telah dilatih.

Fungsi tersebut membuat prediksi rating untuk setiap produk menggunakan model yang telah dilatih, kemudian mengelompokkan produk berdasarkan PRODUCT_ID dan menghitung rata-rata rating prediksi untuk setiap produk. Hasilnya kemudian diurutkan secara menurun berdasarkan rating prediksi, dan lima produk dengan rating tertinggi dipilih sebagai rekomendasi.Akhirnya, program mencetak rekomendasi produk dalam bentuk DataFrame yang berisi PRODUCT_ID dan rating prediksi untuk setiap produk yang direkomendasikan.
"""

# Menggunakan model yang dilatih dengan 100% data latih untuk prediksi
print("\nRekomendasi produk berdasarkan prediksi model (dengan 100% data latih):")
print(rekomendasi_produk_berdasarkan_prediksi(X_encoded, dt_classifier))